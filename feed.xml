<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://roboxiv.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://roboxiv.com/" rel="alternate" type="text/html" /><updated>2025-02-08T18:08:27+00:00</updated><id>https://roboxiv.com/feed.xml</id><title type="html">Minho’s Blog</title><subtitle>Spatial AI and Robotics</subtitle><author><name>Minho Lee</name></author><entry><title type="html">Isaac Lab Tutorial #1 - Introduction</title><link href="https://roboxiv.com/simulation/2025/02/08/isaaclab-tutorial-intro/" rel="alternate" type="text/html" title="Isaac Lab Tutorial #1 - Introduction" /><published>2025-02-08T00:00:00+00:00</published><updated>2025-02-08T00:00:00+00:00</updated><id>https://roboxiv.com/simulation/2025/02/08/isaaclab-tutorial-intro</id><content type="html" xml:base="https://roboxiv.com/simulation/2025/02/08/isaaclab-tutorial-intro/"><![CDATA[<p>📌 최근 로봇 연구에서 <strong>로보틱스 시뮬레이션</strong>은 모델 성능 검증, 학습 워크플로우 구축, 대규모 합성데이터 생성 등 다양한 목적으로 활용되고 있다. 로봇 학습 및 시뮬레이션을 위한 여러 플랫폼이 존재하는 가운데, NVIDIA의 Isaac Lab은 GPU 가속, CUDA, TensorRT 등의 딥러닝 SDK와 Omniverse Isaac Sim을 기반으로 고품질 그래픽과 실시간 레이트레이싱을 지원하는 가상환경을 제공한다. 이러한 특징들은 Embodied AI를 연구하는 연구자들에게 강력하고 효과적인 솔루션이 될 수 있다.</p>

<p>나 역시도 Isaac Lab이 제공하는 기능들을 공부하는 것은 향후 연구에 많은 도움이 될 것이라 생각이 들었다. 그래서, <strong>Isaac Lab의 기본적인 튜토리얼부터 토이 프로젝트를 거쳐 실제 연구 적용까지의 과정을 정리하여 포스팅하려고 한다.</strong></p>

<h3 id="1-before-getting-started">1. Before Getting Started</h3>
<h4 id="11-ngcnvidia-gpu-cloud-container">1.1. NGC(NVIDIA GPU Cloud) Container</h4>
<p align="center"><img src="/assets/img/blog/20250208/fig_1.png" /></p>

<p>NGC에서는 AI, 머신 러닝 및 고성능 컴퓨팅(HPC)을 위한 컨테이너을 제공하며, 사전 학습된 AI 모델과 시각화 도구를 포함한 소프트웨어 스택을 지원한다. AI 모델 구축은 복잡하고 많은 시간이 소요되는데 NGC의 통합 컨테이너 기술은 배포 워크플로우를 간소화하고 AI 프로젝트를 가속화하는 데 도움을 준다.</p>

<p>또한, NGC는 AI 모델 개발에 최적화된 Docker 이미지를 제공하며 TensorFlow, PyTorch 등 주요 프레임워크를 폭넓게 지원하여 GPU 관련 호환성 문제를 해결할 수 있다.</p>

<p>더 자세한 내용은 <a href="https://catalog.ngc.nvidia.com/containers"> NVIDIA NGC Catalog</a>에서 확인할 수 있으며, 안에서 <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/isaac-lab"> NVIDIA Isaac Lab</a>과 관련된 정보도 확인할 수 있다.</p>

<h3 id="2-build-the-isaac-lab-container">2. Build the Isaac Lab container</h3>

<p>Issac Lab 도커 이미지를 빌드하기 위해, Dockerfile과 docker-compose.yml, .env 파일을 포함하는 Root GitHub Repository를 클론한다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git clone https://github.com/isaac-sim/IsaacLab.git
</code></pre></div></div>
<p>클론해온 <mark>IsaacLab</mark> 저장소의 <mark>./docker</mark> 디렉토리에는 Docker 및 ROS2 개발 환경을 구축하고 실행하는 데 필요한 파일들이 위치해 있다.</p>

<p><mark>container.py</mark>와 <mark>container.sh</mark>은 Isaac Lab의 Docker 컨테이너를 쉽게 실행하고 관리할 수 있도록 만들어진 Python 및 Bash 스크립트이다. 이를 활용하여 Docker 이미지를 빌드하고 컨테이너를 실행해보자.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd IsaacLab
$ python3 docker/container.py start
</code></pre></div></div>

<p><mark>container.py</mark>는 Docker 컨테이너를 관리하기 위한 CLI 명령어를 제공한다. 주요 명령어는 다음과 같다.</p>
<ul>
  <li><mark>start</mark>: 도커 이미지를 빌드하고 컨테이너를 백그라운드에서 실행</li>
  <li><mark>enter</mark>: 실행 중인 Isaac Lab 컨테이너에 새 bash 프로세스로 진입</li>
  <li><mark>config</mark>: 제공된 YAML 및 환경 파일을 기반으로 docker-compose.yaml 생성 또는 출력</li>
  <li><mark>copy</mark>: 컨테이너에서 빌드 결과물 및 로그 파일을 호스트로 복사</li>
  <li><mark>stop</mark>: 도커 컨테이너를 종료하고 삭제</li>
</ul>

<p>이미지 빌드가 완료되면 아래 명령어를 통해 컨테이너를 실행할 수 있다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python3 docker/container.py enter
</code></pre></div></div>

<p>이 환경에서는 Isaac Lab 저장소의 사본을 포함하며, Isaac Sim의 디렉토리와 라이브러리에 대한 액세스가 가능하다. 또한, 컨테이너에서는 호스트 장치의 <mark>IsaacLab</mark> 디렉토리가 마운트되어 있기 때문에 호스트에서 해당 디렉토리 아래의 파일을 수정하면 Docker 이미지를 다시 빌드할 필요 없이 변경 사항이 컨테이너에 즉시 반영된다.</p>

<h3 id="3-training-with-an-rl-agent">3. Training with an RL Agent</h3>

<p>이제 컨테이너 안에서 Isaac Lab 루트 디렉토리에 포함된 PPO 에이전트를 Stable-Baseline3를 사용하여 Cartpole balancing task를 해결하는 강화학습 예제를 실행해보고 결과를 확인해보자.</p>

<p>이 튜토리얼과 관련된 더 자세한 내용은 아래 링크를 통해 확인할 수 있다.</p>
<ul>
  <li><a href="https://isaac-sim.github.io/IsaacLab/main/source/tutorials/03_envs/run_rl_training.html"> Isaac Lab Documentation: Training with an RL Agent</a></li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p scripts/reinforcement_learning/sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64 --headless --video --max_iterations 2000

[INFO]: Time taken for scene creation : 1.056965 seconds
[INFO]: Scene manager:  &lt;class InteractiveScene&gt;
	Number of environments: 64
	Environment spacing   : 4.0
	Source prim name      : /World/envs/env_0
	Global prim paths     : []
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
</code></pre></div></div>

<p>에이전트를 훈련하는 세 가지 주요 방법이 있고, 플래그를 통해 설정할 수 있다.</p>

<ol>
  <li>Headless execution: <mark>--headless</mark> 플래그가 설정되면 시뮬레이션은 훈련 중에 렌더링되지 않는다. 일반적으로 물리 시뮬레이션 단계만 수행되기 때문에 훈련 프로세스가 빨라진다.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p scripts/reinforcement_learning/sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64 --headless
</code></pre></div></div>

<ol>
  <li>Headless execution with off-screen render: 위의 headless 실행은 시뮬레이션을 렌더링하지 않기 때문에 훈련 중에 에이전트의 행동을 시각화할 수 없다. 에이전트의 행동을 시각화하기 위해 오프스크린 렌더링을 활성화(<mark>--enable_cameras</mark>)하고 훈련 중에 에이전트의 행동을 녹화(<mark>--video</mark>)하는 플래그를 전달한다.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p scripts/reinforcement_learning/sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64 --headless --video
</code></pre></div></div>

<p>저장된 비디오는 <mark>logs/sb3/Isaac-Cartpole-v0/$run-dir/videos/train</mark> 디렉토리에 저장된다.</p>

<ol>
  <li>Interactive execution: 아래 명령어를 통해 실시간으로 에이전트가 시뮬레이션 환경과 상호 작용하며 훈련되고 있는지 확인할 수 있다. 하지만, 시뮬레이션이 화면에 렌더링되므로 훈련 프로세스가 느려질 수 있다. 해결 방법으로 화면 오른쪽 하단에 도킹된 창에서 다른 렌더링 모드 간에 전환할 수 있다.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p scripts/reinforcement_learning/sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64
</code></pre></div></div>

<p>별도의 터미널에서 아래 명령을 실행하여 학습 진행 상황을 모니터링할 수 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p -m tensorboard.main --logdir logs/sb3/Isaac-Cartpole-v0
</code></pre></div></div>

<p>훈련이 완료되면 다음 명령을 실행하여 훈련된 에이전트를 Isaac Sim에서 시각화할 수 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./isaaclab.sh -p scripts/reinforcement_learning/sb3/play.py --task Isaac-Cartpole-v0 --num_envs 32 --use_last_checkpoint
</code></pre></div></div>

<h3 id="4-trained-agent-visualization">4. Trained Agent Visualization</h3>

<p>에이전트의 훈련이 완료되면 터미널에서 다음과 같은 로그를 확인할 수 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 293          |
|    ep_rew_mean          | 4.82         |
| time/                   |              |
|    fps                  | 6365         |
|    iterations           | 2000         |
|    time_elapsed         | 321          |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0043435125 |
|    clip_fraction        | 0.0512       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0181      |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0119       |
|    n_updates            | 39980        |
|    policy_gradient_loss | -0.00246     |
|    std                  | 0.247        |
|    value_loss           | 0.017        |
------------------------------------------
Saving model checkpoint to /workspace/isaaclab/logs/sb3/Isaac-Cartpole-v0/2025-02-07_07-05-22/model_2048000_steps.zip
</code></pre></div></div>

<p>아래는 오프스크린 렌더링을 사용하여 에이전트의 훈련 중 행동을 스텝 0, 2000, 30000에서 저장된 영상과, 최종 훈련된 모델의 체크포인트를 로드하여 환경에서 에이전트를 실행한 결과를 시각화한 영상이다.</p>

<ul>
  <li>Step 0: Initial agent behavior at the beginning of training</li>
</ul>
<video width="640" height="360" controls="">
  <source src="/assets/img/blog/20250208/rl-video-step-0.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<ul>
  <li>Step 2000: Agent’s progress after 2000 steps of training</li>
</ul>
<video width="640" height="360" controls="">
  <source src="/assets/img/blog/20250208/rl-video-step-2000.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<ul>
  <li>Step 30000: Agent’s behavior after 30000 steps, nearing the end of training</li>
</ul>
<video width="640" height="360" controls="">
  <source src="/assets/img/blog/20250208/rl-video-step-30000.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<ul>
  <li>Final Model Execution: Agent performance after loading the final trained model checkpoint.</li>
</ul>
<video width="640" height="360" controls="">
  <source src="/assets/img/blog/20250208/isaaclab_rl_demo.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<p>다음 포스팅에서는 기본 환경 설정을 다룬 후 강체(rigid object), 관절(articulation), 변형 가능한 객체(deformable object)와의 상호작용에 대한 튜토리얼을 진행할 예정이다.</p>

<hr />]]></content><author><name>Minho Lee</name></author><category term="[&quot;Simulation&quot;]" /><category term="Robotics" /><category term="Isaac Lab" /><category term="Isaac Sim" /><summary type="html"><![CDATA[📌 최근 로봇 연구에서 로보틱스 시뮬레이션은 모델 성능 검증, 학습 워크플로우 구축, 대규모 합성데이터 생성 등 다양한 목적으로 활용되고 있다. 로봇 학습 및 시뮬레이션을 위한 여러 플랫폼이 존재하는 가운데, NVIDIA의 Isaac Lab은 GPU 가속, CUDA, TensorRT 등의 딥러닝 SDK와 Omniverse Isaac Sim을 기반으로 고품질 그래픽과 실시간 레이트레이싱을 지원하는 가상환경을 제공한다. 이러한 특징들은 Embodied AI를 연구하는 연구자들에게 강력하고 효과적인 솔루션이 될 수 있다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://roboxiv.com/assets/img/thumbnails/blog_2.png" /><media:content medium="image" url="https://roboxiv.com/assets/img/thumbnails/blog_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Foundation Models of and for Navigation</title><link href="https://roboxiv.com/study/2025/02/04/foundation-models-of-and-for-navigation/" rel="alternate" type="text/html" title="Foundation Models of and for Navigation" /><published>2025-02-04T00:00:00+00:00</published><updated>2025-02-04T00:00:00+00:00</updated><id>https://roboxiv.com/study/2025/02/04/foundation-models-of-and-for-navigation</id><content type="html" xml:base="https://roboxiv.com/study/2025/02/04/foundation-models-of-and-for-navigation/"><![CDATA[<p>📌 ICRA 2024 Workshop에서 열린 Vision-Language Models for Navigation and Manipulation (VLMNM)를 주제로 한 invited talk 중에서 ViNT, GNM, NoMaD 논문의 저자인 Dhruv Shah 박사님의 <strong>“Foundation Models of and for Navigation”</strong> 발표를 정리해보려고 한다.</p>

<p>워크숍에 대한 정보와 전체 발표 영상은 아래 링크를 통해 확인할 수 있다. <br /></p>
<ul>
  <li><a href="https://vlmnm-workshop.github.io/"> ICRA-24 VLMNM Workshop</a></li>
  <li><a href="https://www.youtube.com/playlist?list=PLvYJV1Xnj1YVZdTrP2i8EqJW7Ai_pIMV7"> Recordings of Invited Talks</a></li>
</ul>

<h3 id="1-foundation-models">1. Foundation Models</h3>
<p>로봇 분야의 최근의 발전을 보면, 중요한 핵심은 대량의 데이터를 활용한 학습에 있다. 다양한 작업에서 수집된 데이터를 통해 모델이 작업의 패턴을 학습하고 이를 활용하여 여러 작업을 수행할 수 있도록 하는 것이다. 이러한 모델들을 <strong>“Foundation Models”</strong>라고 한다. 일반적으로 이러한 Foundation Model들은 광범위한 데이터셋을 사용하여 자가 지도 학습을 통해 학습되며, 최소한의 지도 학습만으로도 뛰어난 일반화 및 적응 능력을 갖는 것이 특징이다.</p>

<p>이러한 모델들은 “인터넷 기반 모델 (Internet Foundation Models)”이라고 할 수도 있는데, 그 이유는 이런 모델들은 능동적으로 데이터를 수집하는 것이 아니라 인터넷에 존재하는 다양한 데이터들을 활용해 훈련되기 때문이다. 이러한 데이터와 모델의 통합은 data-driven robotics에서 중요한 역할을 할 수 있다.</p>

<p>모델을 훈련할 때 단순히 데이터의 규모만으로 충분한 것은 아니다. 모델의 성능을 높이기 위해서는 데이터의 품질과 다양성에 집중하는 데이터 중심적 접근(Data-centric perspective)이 중요하다. 그러나 이러한 고품질의 그리고 다양한 데이터를 수집하는 것은 매우 어렵고 비용이 많이 드는 문제이다.</p>

<p>이러한 관점에서 Dhruv Shah 박사는 <strong>기존에 존재하는 데이터를 활용하여 데이터 기반 로보틱스를 어떻게 실현할 것인지</strong>에 집중해 왔다. 기존 데이터들은 서로 다른 환경과 다양한 작업에서 수집되어 겉보기에는 매우 이질적으로 보일 수 있다. 그러나 적절한 목표와 구조를 정의하면 이러한 데이터를 효과적으로 활용하여 로봇 간 전이 가능한 유용한 표현을 학습할 수 있음을 보여주었다.</p>

<p align="center"><img src="/assets/img/blog/20250204/fig_1.png" /></p>

<h3 id="2-robot-foundation-model">2. Robot Foundation Model</h3>
<p>Robot Foundation Model이란 용어는 많은 의미를 포함하고 있으며, 사람들마다 각자 다른 방식으로 이해할 수도 있다. Dhruv Shah 박사는 연구에서 Robot Foundation Model을 한 번 훈련되면 추가적인 지도 학습 없이도 다양한 로봇에서 바로 사용할 수 있는 모델이라 정의하였다. 이는 서로 다른 센서를 가진 로봇, 전혀 다른 환경에 놓인 로봇에서도 새로운 작업을 수행할 수 있도록 설계된 모델을 의미한다. 이를 위해서 모델이 충족해야 하는 주요 조건은 아래와 같다:</p>

<p align="center"><img src="/assets/img/blog/20250204/fig_2.png" /></p>

<p>이러한 모델을 만들기 위해서는 기존의 개별적인 로봇별 학습 방식에서 벗어나, 여러 로봇에서 수집한 데이터를 통합하여 단일한 거대 신경망 모델을 학습하는 방식 (Cross-Embodiment Learning)이 요구된다.</p>

<h3 id="3-cross-embodiment-learning">3. Cross-Embodiment Learning</h3>
<p>서로 다른 로봇 데이터를 하나의 모델에서 학습시키기 위해서는 특정한 설계 원칙이 필요하다. <br /></p>
<ul>
  <li><strong>공통된 행동 공간(action space) 정의</strong>: 각 로봇은 서로 다른 행동 공간(rotor velocities, joint angles …)을 가진다. 따라서, 이를 통합할 수 있는 공통된 표현이 필요하다.</li>
  <li><strong>로봇의 특성을 반영하는 프롬프트 (Embodiment Prompt)</strong>: 로봇마다 구조나 이동 방식이 다르기 때문에, 모든 로봇에 동일한 방식의 제어 모델을 적용하기 어렵다. 이를 해결하기 위해 로봇의 특성을 명시적인 시스템 파라미터로 직접 학습하는 대신 로봇이 과거에 수행한 행동 데이터를 활용하여 모델이 스스로 로봇의 특성을 학습하도록 하는 접근 방식이 요구된다. 다시 말해, Embodiment Prompt란 로봇의 과거 행동 데이터를 입력으로 제공하여 모델이 해당 로봇의 특성을 자동으로 반영하는 기법을 의미한다.</li>
</ul>

<p>개별 로봇 데이터만 학습한 모델보다, 여러 로봇의 데이터를 통합해 학습한 모델(GNM, General Navigation Model)이 일관되게 높은 성능을 기록함을 보여주었다. 또한, 개별 로봇에서 학습한 정책보다 다수의 로봇 데이터를 결합한 정책이 더욱 일반화된 성능을 보이며, 모델의 크기가 커질 때에도 성능이 더 크게 향상됨을 보였다.</p>

<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/10161227?casa_token=UShbqLF5uogAAAAA:s3h6WmqvbkyN8E8lCbVogL2PXPrGoii7FftHRSKsFySo2NEW0aLQb5dF1A2OLe2KZIligpBP"> Shah, Dhruv, et al. “Gnm: A general navigation model to drive any robot.” 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.</a></li>
</ul>
<p align="center"><img src="/assets/img/blog/20250204/fig_3.gif" /></p>

<h3 id="4-downstream-adaptation">4. Downstream Adaptation</h3>
<p align="center"><img src="/assets/img/blog/20250204/fig_4.png" /></p>

<p>제안하고 있는 Robot Foundation Model을 기반으로 더 고차원의 기능을 추가하기 위한 연구가 진행되고 있다. 주요 연구 방향은 크게 세 가지로 나뉘는데, 기본적으로 학습된 모델은 단순히 목표 지점에 도달하고 충돌을 회피하는 기능만을 수행한다면 기존 모델에 약간의 추가 데이터와 적절한 보상 함수를 사용해 학습시켜 로봇이 사회적 상호작용이 이루어지는 환경에서 더 나은 행동을 학습할 수 있도록 하는 연구가 진행되고 있다.</p>

<p>두 번째로 기존의 모델이 이미지 기반 목표를 사용했다면, 텍스트 기반 목표 또는 GPS 기반 목표를 적용하는 것이 더 유용할 수 있다고 판단하여 연구가 수행되고 있다. 그러나, 텍스트 기반 목표를 적용하려면 모든 데이터를 텍스트로 annotation해야 하는 문제가 있다. 이를 해결하기 위해 이미지 기반 정책을 학습한 후, GPS 좌표나 명령어를 인코딩하는 새로운 모듈을 추가하는 방식을 사용했다고 한다. 이 방식은 GPT 모델에서 소프트 프롬프팅(Soft Prompting)을 사용하는 방식과 유사하며, 향후 연구에서는 In-context Prompting 방식을 활용해 더욱 효과적인 목표 설정을 탐구할 예정이라고 한다.</p>

<p>마지막으로 대규모의 로봇 데이터셋 구축(Open Cross-Embodiment Collaboration)이 진행 중이다. 연구 결과에서 더 다양한 로봇 데이터를 통합할 수록 Positive Transfer 현상이 발생함을 확인하였으며, 특히 manipulation과 navigation tasks를 함께 정책으로 학습할 경우에 두 작업 모두에서 성능이 개선되는 효과가 나타났다. 이러한 연구를 바탕으로 더 다양한 로봇과 작업 데이터를 동시에 학습하는 하나의 통합된 모델로의 확장 연구가 진행되고 있다고 한다.</p>

<h3 id="5-conclusion">5. Conclusion</h3>
<p>발표 내용을 요약하면 다음과 같다:</p>

<ul>
  <li>로봇 분야에서 <u>Foundation Models</u>은 일반적으로 대량의 데이터를 활용하여 로봇이 다양한 작업의 패턴을 학습하고, 이를 통해 여러 작업을 수행 가능하도록 하는 모델을 의미한다.</li>
  <li>모델을 훈련할 때 데이터의 규모만으로 충분하지 않으며, 데이터의 품질과 다양성에 집중하는 <u>Data-centric Perspective</u>가 중요하다. 하지만, 고품질 데이터를 확보하는 것은 높은 비용과 어려움이 따르는 문제이다.</li>
  <li>Dhruv Shah 박사는 연구에서 <u>Robot Foundation Model</u>을 추가적인 지도 학습 없이도 다양한 로봇에서 바로 사용 가능하며, 이종 센서나 새로운 환경에서도 적용될 수 있도록 설계된 모델이라 정의했다.</li>
  <li>위의 모델을 설계하기 위해서는 <u>Cross-Embodiment Learning</u>의 개념이 필요하며, 이는 서로 다른 로봇의 행동 공간을 통합하고 로봇의 특성을 반영하는 프롬프트를 활용하여 모델을 학습하는 방식이다. 이를 통해 개별적인 학습보다, 보다 일관적이고 일반화된 성능을 보인다는 것이 실험적으로 입증되었다.</li>
  <li>향후 연구에서는 기존 모델에 <u>사회적 보상함수 추가</u>, <u>새로운 모달리티(텍스트/GPS 기반 목표) 적용</u>, <u>더 많은 로봇 모델 및 작업을 학습한 통합 모델로의 확장</u>의 방향이 진행될 것이다.</li>
</ul>

<p>끝으로, 앞서 언급된 모델 및 관련 프레임워크는 아래 GitHub에서 확인할 수 있다.</p>
<ul>
  <li><a href="https://github.com/robodhruv/visualnav-transformer"> Berkeley AI Research &amp; Dhruv Shah Github</a></li>
</ul>

<p>내가 연구하는 필드에서 Dhruv Shah 박사님의 연구들은 큰 주목을 받았었고, 현재도 많은 관심을 받고 있다. 앞으로도 의미 있는 연구들이 나올 것으로 기대되기 때문에, 계속해서 살펴볼 예정이다.
그리고 GitHub에 공개된 사전 학습된 모델을 연구실의 로봇 플랫폼에 적용하고 다양한 모듈과 연계하여 실험해 볼 계획인데, 아주 흥미로운 작업이 될 것 같다!</p>

<hr />]]></content><author><name>Minho Lee</name></author><category term="[&quot;Study&quot;]" /><category term="ICRA2024" /><category term="Workshop" /><category term="Foundation Model" /><category term="Robotics" /><category term="Navigation" /><summary type="html"><![CDATA[📌 ICRA 2024 Workshop에서 열린 Vision-Language Models for Navigation and Manipulation (VLMNM)를 주제로 한 invited talk 중에서 ViNT, GNM, NoMaD 논문의 저자인 Dhruv Shah 박사님의 “Foundation Models of and for Navigation” 발표를 정리해보려고 한다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://roboxiv.com/assets/img/thumbnails/blog_1.png" /><media:content medium="image" url="https://roboxiv.com/assets/img/thumbnails/blog_1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>